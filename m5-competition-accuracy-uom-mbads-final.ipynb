{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "Welcome to our notebook. The main goal of this notebook is to find the best solution for the “M5 Competition - Accuracy”. Specifically, we will try to predict the future sales for Walmart based on hierarchical sales data, generously made available by Walmart, starting at the item level and aggregating to that of departments, product categories and stores in three geographical areas of the US: California, Texas, and Wisconsin.\n",
    "\n",
    "In this Kernel, we will explain each stage of analysis, such as cleaning and editing the existing data. Then, we will apply 3 different machine learning algorithms in order to achieve the best results in terms of accuracy. More specific, we will use Facebook Prophet, Random Forest Regression and XGBoost.\n",
    "\n",
    "**What is Facebook Prophet?** Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.\n",
    "\n",
    "**What is Random Forest Regression** A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "**What is XGBoost** XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.\n",
    "\n",
    "**Dataset** The dataset that we will apply the machine learning models constitutes sales data that they were provided by Walmart. More specific, the M5 dataset contains sales data for 3,049 different products, classified in 3 product categories (Hobbies, Foods and Household).\n",
    "\n",
    "**Data Sources Explanation** In order to proceed in the analysis, 3 different files are provided:\n",
    "\n",
    "sell_prices.csv: Contains information about the price of the products sold per store and date. sales_train.csv: Contains the historical daily unit sales data per product and store. calendar: Contains information about the dates the products are sold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to import all the necessary libraries and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, Dropdown\n",
    "from ipywidgets import Layout\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from fbprophet import Prophet\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand the problem we need to read the data from each data source. Thus, we will have a better view of the data that will help us to schedule our actions properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#DATASET WITH DATES\n",
    "calendar = pd.read_csv(\"calendar.csv\")\n",
    "\n",
    "#DATASET WITH SALES\n",
    "sales_eval = pd.read_csv(\"sales_train_evaluation.csv\")\n",
    "\n",
    "#DATASET WITH THE NEXT 28 DAILY SALES\n",
    "sales = pd.read_csv(\"sales_train_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  wm_yr_wk    weekday  wday  month  year    d event_name_1  \\\n",
      "0  2011-01-29     11101   Saturday     1      1  2011  d_1          NaN   \n",
      "1  2011-01-30     11101     Sunday     2      1  2011  d_2          NaN   \n",
      "2  2011-01-31     11101     Monday     3      1  2011  d_3          NaN   \n",
      "3  2011-02-01     11101    Tuesday     4      2  2011  d_4          NaN   \n",
      "4  2011-02-02     11101  Wednesday     5      2  2011  d_5          NaN   \n",
      "\n",
      "  event_type_1 event_name_2 event_type_2  snap_CA  snap_TX  snap_WI  \n",
      "0          NaN          NaN          NaN        0        0        0  \n",
      "1          NaN          NaN          NaN        0        0        0  \n",
      "2          NaN          NaN          NaN        0        0        0  \n",
      "3          NaN          NaN          NaN        1        1        0  \n",
      "4          NaN          NaN          NaN        1        0        1  \n"
     ]
    }
   ],
   "source": [
    "#CALENDAR HEAD\n",
    "print(calendar.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              id        item_id    dept_id   cat_id store_id  \\\n",
      "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
      "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
      "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
      "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
      "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
      "\n",
      "  state_id  d_1  d_2  d_3  d_4  ...  d_1932  d_1933  d_1934  d_1935  d_1936  \\\n",
      "0       CA    0    0    0    0  ...       2       4       0       0       0   \n",
      "1       CA    0    0    0    0  ...       0       1       2       1       1   \n",
      "2       CA    0    0    0    0  ...       1       0       2       0       0   \n",
      "3       CA    0    0    0    0  ...       1       1       0       4       0   \n",
      "4       CA    0    0    0    0  ...       0       0       0       2       1   \n",
      "\n",
      "   d_1937  d_1938  d_1939  d_1940  d_1941  \n",
      "0       0       3       3       0       1  \n",
      "1       0       0       0       0       0  \n",
      "2       0       2       3       0       1  \n",
      "3       1       3       0       2       6  \n",
      "4       0       0       2       1       0  \n",
      "\n",
      "[5 rows x 1947 columns]\n"
     ]
    }
   ],
   "source": [
    "#SALES HEAD\n",
    "print(sales_eval.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to eliminate duplicate values. So we need to aggregate all the products in order to create a file with unique products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       item_id  d_1  d_2  d_3  d_4  d_5  d_6  d_7  d_8  d_9  ...  d_1932  \\\n",
      "0  FOODS_1_001    6    6    4    6    7   18   10    4   11  ...       7   \n",
      "1  FOODS_1_002    4    5    7    4    3    4    1    7    2  ...       5   \n",
      "2  FOODS_1_003   14    8    3    6    3    8   13   10   11  ...       7   \n",
      "3  FOODS_1_004    0    0    0    0    0    0    0    0    0  ...     115   \n",
      "4  FOODS_1_005   34   32   13   20   10   21   18   20   25  ...      15   \n",
      "\n",
      "   d_1933  d_1934  d_1935  d_1936  d_1937  d_1938  d_1939  d_1940  d_1941  \n",
      "0       8       8       4       7       7       5       7       5       9  \n",
      "1       8       7       2       6       5       0       6       6       4  \n",
      "2       8       6      11       9       7       7      10       6       5  \n",
      "3     104     107      81      73      62      71      75      83      93  \n",
      "4      16      25       9       7      25      19      25      17      19  \n",
      "\n",
      "[5 rows x 1942 columns]\n"
     ]
    }
   ],
   "source": [
    "#CREATE FILE WITH ALL PRODUCTS REGARDLESS OF THE ANNEX SO THAT WE DONT HAVE DUPLICATE PRODUCTS\n",
    "sales_final=sales.groupby('item_id',as_index=False).sum()\n",
    "sales_final['total']=sales_final.iloc[:,1:].sum(axis=1)\n",
    "\n",
    "#SORT THE FINAL FILE IN DESCENDING ORDER TO APPEAR IN THE DROP DOWN MENU BASED ON SALES \n",
    "sales_total_sort=sales_final.iloc[:,[0,-1]].sort_values(by='total',ascending=False)\n",
    "\n",
    "#GROUP BY THE FILE WITH THE REAL 28 NEXT DAILY SALES\n",
    "sales_eval_final=sales_eval.groupby('item_id',as_index=False).sum()\n",
    "\n",
    "print(sales_eval_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we will split the file that we created based on the product categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            item_id   total\n",
      "1799  HOBBIES_1_371  122565\n",
      "1776  HOBBIES_1_348  105876\n",
      "1697  HOBBIES_1_268   95171\n",
      "1769  HOBBIES_1_341   92427\n",
      "1610  HOBBIES_1_178   90433\n",
      "              item_id   total\n",
      "2329  HOUSEHOLD_1_334  171275\n",
      "2451  HOUSEHOLD_1_459  170532\n",
      "2513  HOUSEHOLD_1_521  153340\n",
      "2298  HOUSEHOLD_1_303  148818\n",
      "2108  HOUSEHOLD_1_110  146719\n",
      "          item_id    total\n",
      "702   FOODS_3_090  1002529\n",
      "1198  FOODS_3_586   920242\n",
      "864   FOODS_3_252   565299\n",
      "1167  FOODS_3_555   491287\n",
      "1325  FOODS_3_714   396172\n"
     ]
    }
   ],
   "source": [
    "#SPLIT  DATASET BASED ON THE CATEGORIES OF THE PRODUCTS &\n",
    "#SORT THEM IN DESCENDING ORDER TO APPEAR IN THE DROP DOWN MENU BASED ON SALES\n",
    "\n",
    "#HOBBIES\n",
    "hobbies = sales_final[sales_final['item_id'].str.split('_').str[0]=='HOBBIES']\n",
    "hobbies.shape\n",
    "\n",
    "hobbies_sort=hobbies.iloc[:,[0,-1]].sort_values(by='total',ascending=False)\n",
    "print(hobbies_sort.head())\n",
    "\n",
    "#HOUSEHOLD\n",
    "household = sales_final[sales_final['item_id'].str.split('_').str[0]=='HOUSEHOLD']\n",
    "household.shape\n",
    "\n",
    "household_sort=household.iloc[:,[0,-1]].sort_values(by='total',ascending=False)\n",
    "print(household_sort.head())\n",
    "\n",
    "#FOODS\n",
    "foods = sales_final[sales_final['item_id'].str.split('_').str[0]=='FOODS']\n",
    "foods.shape\n",
    "\n",
    "foods_sort=foods.iloc[:,[0,-1]].sort_values(by='total',ascending=False)\n",
    "print(foods_sort.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a few functions in order to better manage the data. Thus, it will be easier to build the models properly without any mistakes. So, as we can easily understand, this procedure is really vital."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we created a function for the date features, such as date, month, year. Thus, it is easier to call each of them when it is neccessary. Actually, we created a variable X which containts the dataframe with the date features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE A FUNCTION TO ADD FEATURES ON TRAIN SET BASED ON DATES\n",
    "def date_features(df, label=None,r=None): #we use the variable r to define the range of days\n",
    "    df = df.copy()\n",
    "\n",
    "    df['date'] = df.Date\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['dayofweek'] = [i for i in calendar['wday'][:r]]\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['dayofyear'] = df['date'].dt.dayofyear\n",
    "    df['dayofmonth'] = df['date'].dt.day\n",
    "    df['weekofyear'] = df['date'].dt.isocalendar().week.astype('int64')\n",
    "    df['Event_1']=np.where(calendar['event_type_1'][:r].isna(),0,1)\n",
    "    df['Event_2']=np.where(calendar['event_type_2'][:r].isna(),0,1)\n",
    "    #SATURDAY OR SUNDAY\n",
    "    df['S&S']=[1 if calendar.iloc[i,3]==1 else 1 if calendar.iloc[i,3]==2 else 0 for i in range(len(calendar[:r]))]\n",
    "    \n",
    "    X = df[['dayofweek','quarter','month','year',\n",
    "           'dayofyear','dayofmonth','weekofyear','Event_1','Event_2','S&S']]\n",
    "    if label:\n",
    "        y = df[label]\n",
    "        return X, y\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE A FUNCTION TO ADD FEATURES ON TEST SET BASED ON DATES\n",
    "def date_features_next28(df, label=None,r1=None,r2=None): #we use r1 and r2 variables to define the next 28 days\n",
    "    df = df.copy()\n",
    "\n",
    "    df['date'] = df.Date\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['dayofweek'] = [i for i in calendar['wday'][r1:r2]] \n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['dayofyear'] = df['date'].dt.dayofyear\n",
    "    df['dayofmonth'] = df['date'].dt.day\n",
    "    df['weekofyear'] = df['date'].dt.isocalendar().week.astype('int64')\n",
    "    df['Event_1']=np.where(calendar['event_type_1'][r1:r2].isna(),0,1)\n",
    "    df['Event_2']=np.where(calendar['event_type_2'][r1:r2].isna(),0,1)\n",
    "    #SATURDAY OR SUNDAY\n",
    "    df['S&S']=[1 if calendar.iloc[i,3]==1 else 1 if calendar.iloc[i,3]==2 else 0 for i in range(len(calendar[r1:r2]))]\n",
    "    \n",
    "    X = df[['dayofweek','quarter','month','year',\n",
    "           'dayofyear','dayofmonth','weekofyear','Event_1','Event_2','S&S']]\n",
    "    if label:\n",
    "        y = df[label]\n",
    "        return X, y\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to mention that this Kernel is dynamic. Specifically, we created a drop down menu in order to give users the capability to choose between various reports/results. To be more specific, the choices are: XGBoost, FB Prophet, RandomForest, Plot Times Series, See Trends, XGBoost2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE THE DROP DOWN WITH THE PRODUCTS CATEGORIES, RANGE OF SALES, THE PRODUCT, VISUALIZATIONS AND PREDICTIONS ALGORITHMS\n",
    "\n",
    "#CHOICES OF DROP DOWN MENU\n",
    "\n",
    "cat={'HOBBIES':hobbies_sort,'HOUSEHOLD':household_sort,'FOODS':foods_sort,'TOTAL':sales_total_sort}\n",
    "opt=sales_total_sort.sort_values('total')['total'][:]\n",
    "AlgoChoices={3:\"Select\",4:\"Plot Time Series\",5:\"See Trends\",6:\"Plot Total Sales\",0:\"FB Prophet\",1:\"XGBoost\",2:\"XGBoost2\",7:\"RandomForest\"}\n",
    "\n",
    "catW=widgets.Dropdown(options = cat.keys(),description='Category:')\n",
    "\n",
    "#RANGE OF SALES\n",
    "range_slider=widgets.SelectionRangeSlider(options=opt,index=(200,2000),layout=Layout(width='50%'),step=100,description='Sales Number',disabled=False, feature_weights=True)\n",
    "\n",
    "idW=widgets.Dropdown(description='Product:')\n",
    "\n",
    "algoChoice=widgets.Dropdown(options=AlgoChoices.values(),description='Algorithm:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you will find the function which is important in order to call the option \"Plot Time Series\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DAILY SALES FOR EACH PRODUCT\n",
    "def Plot_TimeSeries():\n",
    "    \n",
    "    df_sales_temp = sales_final[sales_final['item_id']==idW.value].iloc[:,1:-1].sum(axis=0)\n",
    "    df_sales_temp.index = range(1913)\n",
    "    \n",
    "    Sales_dict ={'Sales' : df_sales_temp,'Date' : pd.to_datetime(calendar['date'][:-56])}\n",
    "   \n",
    "    product_sales_per_date = pd.DataFrame(Sales_dict)\n",
    "    \n",
    "    #DAILY SALES PLOT\n",
    "    sns.set(font_scale=2)  # big\n",
    "    fig, ax = plt.subplots(figsize=(40,14))\n",
    "    a = sns.lineplot(x=\"Date\", y=\"Sales\", data=product_sales_per_date)\n",
    "    a.set_title(\"Daily Sales Data\",fontsize=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you will find the function which is important in order to call the option \"See Trends\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRENDS\n",
    "def See_Trends():\n",
    "    \n",
    "    df_sales_temp = sales_final[sales_final['item_id']==idW.value].iloc[:,1:-1].sum(axis=0)\n",
    "    df_sales_temp.index = range(1913)   \n",
    "    \n",
    "    Sales_dict ={'Sales' : df_sales_temp,'Date' : pd.to_datetime(calendar['date'][:-56])}\n",
    "    \n",
    "    product_sales_per_date = pd.DataFrame(Sales_dict)\n",
    "    \n",
    "    #Call the Function\n",
    "    X, y = date_features(product_sales_per_date, label='Sales',r=1913)\n",
    "    df_new = pd.concat([X, y], axis=1)\n",
    "    df_new.head()\n",
    "\n",
    "    #Plotting the Features to see trends (SALES PER MONTH)\n",
    "    sns.set(font_scale=2)  # big\n",
    "    fig, ax = plt.subplots(figsize=(28,10))\n",
    "    palette = sns.color_palette(\"mako_r\", 4)\n",
    "    a = sns.barplot(x=\"month\", y=\"Sales\",hue = 'year',data=df_new)\n",
    "    a.set_title(\"Store Sales Data\",fontsize=15)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you will find the function which is important in order to call \"Plot Total Sales\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT FOR TOTAL SALES\n",
    "def Plot_Total_Sales():\n",
    "    \n",
    "    df_sales_temp = sales_final[sales_final['item_id']==idW.value].iloc[:,1:-1].sum(axis=0)\n",
    "    df_sales_temp.index = range(1913)   \n",
    "    \n",
    "    Sales_dict ={'Sales' : df_sales_temp,'Date' : pd.to_datetime(calendar['date'][:-56])}\n",
    "    \n",
    "    product_sales_per_date = pd.DataFrame(Sales_dict)   \n",
    "    \n",
    "    #Call the Function\n",
    "    X, y = date_features(product_sales_per_date, label='Sales',r=1913)\n",
    "    df_new = pd.concat([X, y], axis=1)\n",
    "    df_new.head()\n",
    "    \n",
    "    fig,(ax1,ax2,ax3,ax4)= plt.subplots(nrows=4)\n",
    "    fig.set_size_inches(30,45)\n",
    "    \n",
    "    monthAggregated = pd.DataFrame(df_new.groupby(\"month\")[\"Sales\"].sum()).reset_index().sort_values('Sales')\n",
    "    sns.barplot(data=monthAggregated,x=\"month\",y=\"Sales\",ax=ax1)\n",
    "    ax1.set(xlabel='Month', ylabel='Total Sales received')\n",
    "    ax1.set_title(\"Total Sales received By Month\",fontsize=15)\n",
    "\n",
    "    monthAggregated = pd.DataFrame(df_new.groupby(\"dayofweek\")[\"Sales\"].sum()).reset_index().sort_values('Sales')\n",
    "    sns.barplot(data=monthAggregated,x=\"dayofweek\",y=\"Sales\",ax=ax2)\n",
    "    ax2.set(xlabel='dayofweek', ylabel='Total Sales received')\n",
    "    ax2.set_title(\"Total Sales received By Weekday\",fontsize=15)\n",
    "\n",
    "    monthAggregated = pd.DataFrame(df_new.groupby(\"quarter\")[\"Sales\"].sum()).reset_index().sort_values('Sales')\n",
    "    sns.barplot(data=monthAggregated,x=\"quarter\",y=\"Sales\",ax=ax3)\n",
    "    ax3.set(xlabel='Quarter', ylabel='Total Sales received')\n",
    "    ax3.set_title(\"Total Sales received By Quarter\",fontsize=15)\n",
    "\n",
    "    monthAggregated = pd.DataFrame(df_new.groupby(\"year\")[\"Sales\"].sum()).reset_index().sort_values('Sales')\n",
    "    sns.barplot(data=monthAggregated,x=\"year\",y=\"Sales\",ax=ax4)\n",
    "    ax4.set(xlabel='year', ylabel='Total Sales received')\n",
    "    ax4.set_title(\"Total Sales received By year\",fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you will find the function which is vital for FB Prophet's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION FOR FBPROPHET \n",
    "def my_Prophet():\n",
    "    \n",
    "    df_sales_temp = sales_final[sales_final['item_id']==idW.value].iloc[:,1:-1].sum(axis=0)\n",
    "    df_sales_temp.index = range(1913)       \n",
    "    \n",
    "    Sales_dict ={'y' : df_sales_temp,'ds' : pd.to_datetime(calendar['date'][:-56])}\n",
    "    \n",
    "    product_sales_per_date = pd.DataFrame(Sales_dict)\n",
    "    \n",
    "    #SPLIT THE DATA SET\n",
    "    split_date = '2016-02-01'\n",
    "    subset1 = (product_sales_per_date['ds'] <= split_date)\n",
    "    subset2 = (product_sales_per_date['ds'] > split_date)\n",
    "\n",
    "\n",
    "    X_tr = product_sales_per_date.loc[subset1]\n",
    "    X_tst = product_sales_per_date.loc[subset2]\n",
    "    print(\"train shape\",X_tr.shape)\n",
    "    print(\"test shape\",X_tst.shape)\n",
    "\n",
    "    pd.plotting.register_matplotlib_converters()\n",
    "    f, ax = plt.subplots(figsize=(42,15))\n",
    "    X_tr.plot(kind='line', x='ds', y='y', color='blue', label='Train', ax=ax)\n",
    "    X_tst.plot(kind='line', x='ds', y='y', color='red', label='Test', ax=ax)\n",
    "    plt.title('Sales Amount Traning and Test data')\n",
    "    plt.show()\n",
    "    model =Prophet()\n",
    "    model.add_country_holidays(country_name='US')\n",
    "    model.add_seasonality(name='custom_seasonality', period=28,fourier_order=5)\n",
    "    model.fit(X_tr)\n",
    "\n",
    "    \n",
    "    future_dates = pd.DataFrame({'ds':pd.date_range('2016-04-24', periods=28)})\n",
    "    forecast = model.predict(future_dates)\n",
    "    forecast[['ds', 'yhat']]\n",
    "    \n",
    "    # Plot the components of the model\n",
    "    fig = model.plot_components(forecast,figsize=(45,15))\n",
    "\n",
    "    # Plot the forecast\n",
    "    f, ax = plt.subplots(1)\n",
    "    f.set_figheight(15)\n",
    "    f.set_figwidth(45)\n",
    "    fig = model.plot(forecast,ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Plot the forecast with the actuals\n",
    "    f, ax = plt.subplots(1)\n",
    "    f.set_figheight(15)\n",
    "    f.set_figwidth(45)\n",
    "    ax.scatter(X_tst.ds, X_tst['y'], color='r')\n",
    "    fig = model.plot(forecast, ax=ax)\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(14,5))\n",
    "    f.set_figheight(15)\n",
    "    f.set_figwidth(45)\n",
    "    X_tst.plot(kind='line',x='ds', y='y', color='red', label='Test', ax=ax)\n",
    "    forecast.plot(kind='line',x='ds',y='yhat', color='green',label='Forecast', ax=ax)\n",
    "    plt.title('Forecast vs Actuals')\n",
    "    plt.show()\n",
    "    \n",
    "    real_sales=sales_eval_final[sales_eval_final['item_id']==idW.value].iloc[:,1914:].sum(axis=0).values\n",
    "    rmse=np.sqrt(mean_squared_error(real_sales, forecast['yhat']))\n",
    "\n",
    "    print(forecast[['ds', 'yhat']])\n",
    "    print(\"RMSE\",rmse)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differentiate of the dataset between train and test its a very common step in order to apply the ML models properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, n_test):\n",
    "    \n",
    "    return data[:-n_test, :], data[-n_test:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply XGBoost in order to forecast the next 28 days for a certain products based only on \n",
    "previous historical sales. In order to apply this method properly is needed to tranform time series data to supervised. Then, we will use a function called \"walk_forward_validation\" which is responsible for the estimation of RMSE and MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast monthly sales with xgboost\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[0]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols = list()\n",
    "    \n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "    \n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    \n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "        \n",
    "    return agg.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_forecast(train, testX):\n",
    "    \n",
    "    # transform list into array\n",
    "    train = np.asarray(train)\n",
    "    \n",
    "    # split into input and output columns\n",
    "    trainX, trainy = train[:, :-1], train[:, -1]\n",
    "    \n",
    "    # fit model\n",
    "    model= XGBRegressor(n_estimators=125,learning_rate=0.01,gamma=0.01,reg_lambda=0.01,max_depth=7)\n",
    "    model.fit(trainX, trainy)\n",
    "    # make a one-step prediction\n",
    "    yhat = model.predict(np.asarray([testX]))\n",
    "    \n",
    "    return yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(data, n_test):\n",
    "    \n",
    "    predictions = list()\n",
    "    \n",
    "    # split dataset\n",
    "    train, test = train_test_split(data, n_test)\n",
    "    \n",
    "    # seed history with training dataset\n",
    "    history = [x for x in train]\n",
    "    \n",
    "    # step over each time-step in the test set\n",
    "    for i in range(len(test)):\n",
    "        \n",
    "        # split test row into input and output columns\n",
    "        testX, testy = test[i, :-1], test[i, -1]\n",
    "   \n",
    "        # fit model on history and make a prediction\n",
    "        yhat = xgboost_forecast(history, testX)\n",
    "         # store forecast in list of predictions\n",
    "        predictions.append(yhat)\n",
    "        \n",
    "        # add actual observation to history for the next loop\n",
    "        history.append(test[i])\n",
    "        \n",
    "        # summarize progress\n",
    "        print('>expected=%.1f, predicted=%.1f' % (testy, yhat))\n",
    "    \n",
    "    # estimate prediction error\n",
    "    mae=mean_absolute_error(test[:, -1], predictions)\n",
    "    rmse=np.sqrt(mean_squared_error(test[:, -1], predictions))\n",
    "    \n",
    "    return mae,rmse, test[:, -1], predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoost():\n",
    "    \n",
    "    df_sales_temp = sales_final[sales_final['item_id']==idW.value].iloc[:,1:-1].sum(axis=0)\n",
    "    df_sales_temp.index = range(1913)\n",
    "    \n",
    "    Sales_dict ={'Sales' : df_sales_temp,'Date' : pd.to_datetime(calendar['date'][:-28])}\n",
    "    \n",
    "    product_sales_per_date = pd.DataFrame(Sales_dict)\n",
    "    \n",
    "    X,y=date_features(product_sales_per_date, label='Sales',r=1941)\n",
    "    \n",
    "    X=X.set_index(product_sales_per_date['Date'])\n",
    "    series=X\n",
    "    values=y\n",
    "    data = series_to_supervised(values, n_in=6, n_out=1)\n",
    "    mae,rmse ,y, yhat = walk_forward_validation(data,28)\n",
    "    \n",
    "    return (\"MAE\",round(mae,4)), (\"RMSE\",rmse)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply XGBoost (XGBoost2) for a second time. However, we will use some extra features such as Year or Dayofweek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoost2():\n",
    "    \n",
    "    df_sales_temp = sales_final[sales_final['item_id']==idW.value].iloc[:,1:-1].sum(axis=0)\n",
    "    df_sales_temp.index = range(1913)\n",
    " \n",
    "    Sales_dict ={'Sales' : df_sales_temp,'Date' : pd.to_datetime(calendar['date'][:-56])}\n",
    "    \n",
    "    product_sales_per_date = pd.DataFrame(Sales_dict)\n",
    "    X,y=date_features(product_sales_per_date, label='Sales',r=1913)\n",
    "\n",
    "    #split the dataset at 70-30 \n",
    "    df_train=X.iloc[:-574,:]\n",
    "    df_test=X.iloc[-574:,:] \n",
    "\n",
    "    X_train=df_train\n",
    "    y_train=y[:-574]\n",
    "    x_test=df_test\n",
    "    y_test=y[-574:]\n",
    "\n",
    "    # Initialize XGB and GridSearch\n",
    "    xgb_reg = xgb.XGBRegressor(colsample_bytree= 0.7,subsample=0.4,reg_alpha=0.7,min_child_weight=30,n_estimators=50,learning_rate=0.1,gamma=0.7,reg_lambda=1.5,max_depth=3)\n",
    "    xgb_reg.fit(X_train,y_train)\n",
    "    pred_xgb=xgb_reg.predict(x_test)\n",
    "    \n",
    "    #NEW DATASET(NEXT 28 DAYS)\n",
    "    df_test28next_xgb=pd.DataFrame()\n",
    "    df_test28next_xgb['Date']=pd.to_datetime(calendar[calendar['date']>\"2016-04-24\"]['date'][:28])\n",
    "    X_new=date_features_next28(df_test28next_xgb,r1=1913,r2=1941)\n",
    "    \n",
    "    #PREDICTIONS FOR NEXT 28 DAYS\n",
    "    pred2=xgb_reg.predict(X_new)\n",
    "    \n",
    "    #ADD THE PREDICTIONS AND THE REAL SALES AT TEST DATA SET\n",
    "    X_new['Predicted Sales']=abs(pred2.round(0))\n",
    "    X_new['Real Sales']=sales_eval_final[sales_eval_final['item_id']==idW.value].iloc[:,1914:].sum(axis=0).values\n",
    "   \n",
    "    #THE TEST DATA SET WITH PREDICTIONS AND REAL SALES\n",
    "    print(X_new)\n",
    "    \n",
    "    #RMSE\n",
    "    rmse=np.sqrt(mean_squared_error(y_test, pred_xgb))\n",
    "    print(\"RMSE\",rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to predict the sales for the next 28 days, we will use RandomForest, too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest():\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    df_sales_temp = sales_final[sales_final['item_id']==idW.value].iloc[:,1:-1].sum(axis=0)\n",
    "    df_sales_temp.index = range(1913)\n",
    "\n",
    "    Sales_dict ={'Sales' : df_sales_temp,'Date' : pd.to_datetime(calendar['date'][:-56])}\n",
    "    product_sales_per_date = pd.DataFrame(Sales_dict)\n",
    "    \n",
    "    X, y= date_features(product_sales_per_date, label='Sales',r=1913)\n",
    "    \n",
    "    Dataframe_RandomForest = pd.concat([X, y], axis=1)\n",
    "    predictors=Dataframe_RandomForest.drop(['Sales'],axis=1)\n",
    "    target=Dataframe_RandomForest['Sales']\n",
    "    X_train,X_test_cv,y_train,y_test_cv=train_test_split(predictors, target, test_size=0.25,random_state=42)\n",
    "    \n",
    "    #Hypertuned Model\n",
    "    model = RandomForestRegressor(n_estimators=100,oob_score=True,n_jobs =1,random_state=42,max_features='auto',min_samples_leaf=4)\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    #PREDICTION TO TRAIN SER\n",
    "    pred=model.predict(X_test_cv)\n",
    "    \n",
    "    #NEW DATASET(NEXT 28 DAYS)\n",
    "    df_test28next=pd.DataFrame()\n",
    "    df_test28next['Date']=pd.to_datetime(calendar[calendar['date']>\"2016-04-24\"]['date'][:28])\n",
    "    X_test_28=date_features_next28(df_test28next,r1=1913,r2=1941)\n",
    "    \n",
    "    #PREDICTIONS FOR NEXT 28 DAYS\n",
    "    pred2=model.predict(X_test_28)\n",
    "    \n",
    "    #ADD THE PREDICTIONS AND THE REAL SALES AT TEST DATA SET\n",
    "    X_test_28['Predicted Sales']=pred2.round(0)\n",
    "    X_test_28['Real Sales']=sales_eval_final[sales_eval_final['item_id']==idW.value].iloc[:,1914:].sum(axis=0).values\n",
    "    \n",
    "    #TEST DATA SET\n",
    "    print(X_test_28)\n",
    "    \n",
    "    #RMSE\n",
    "    rmse=np.sqrt(mean_squared_error(y_test_cv, pred))\n",
    "    print(\"RMSE\",rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a drop down menu in order to give the user some choices regarding ML models & plots. Below, you can see the available choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MENU\n",
    "def Choice(A):\n",
    "    if A==3:\n",
    "        return (\"Select\")\n",
    "    elif A==6:\n",
    "        return Plot_Total_Sales()\n",
    "    elif A==5:\n",
    "        return See_Trends()\n",
    "    elif A==4:\n",
    "        return Plot_TimeSeries()\n",
    "    elif A==0:    \n",
    "        return my_Prophet()\n",
    "    elif A==1:\n",
    "        return XGBoost()\n",
    "    elif A==2:\n",
    "        return XGBoost2()\n",
    "    elif A==7:\n",
    "        return RandomForest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interact is used for the dropdown creation. Specifically, the user should first choose a product category, the sales range using a slidebar, the product that he/she wants to make the prediction and finally the ML model that he/she wants to apply or the plot that he/she wants to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef59d2b1c72644f3a9889b65585164f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Category:', options=('HOBBIES', 'HOUSEHOLD', 'FOODS', 'TOTAL'), va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(Category=catW,RS=range_slider,Product=idW, AC=algoChoice)\n",
    "def print_idW(Category,RS,Product,AC):\n",
    "    idW.options=cat[Category][(cat[Category]['total']>=RS[0]) & (cat[Category]['total']<=RS[1])]['item_id']\n",
    "    return Choice(list(AlgoChoices.keys())[list(AlgoChoices.values()).index(AC)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
